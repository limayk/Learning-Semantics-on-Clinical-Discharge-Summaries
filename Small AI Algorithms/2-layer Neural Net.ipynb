{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee5d720-857c-4242-99de-3286458162d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed222c-50fa-481d-874c-6ffde954fc0d",
   "metadata": {},
   "source": [
    "### helper functions\n",
    "largely for plotting and calculating error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7d244-a33f-4719-b195-012b4102a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_intercept(x):\n",
    "\n",
    "    with_intercept = np.zeros((len(x), len(x[0])+1), dtype = x.dtype)\n",
    "    with_intercept[:, 0] = 1\n",
    "    with_intercept[:, 1:] = x\n",
    "\n",
    "    return with_intercept\n",
    "\n",
    "def plot_training(list1, name_of_list1, list2, name_of_list2, save_path):\n",
    "    x_values = np.linspace(1, len(list1), len(list1))\n",
    "    plt.plot(x_values, list1, label=name_of_list1)\n",
    "    plt.plot(x_values, list2, label=name_of_list2)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def print_results(error_rates, reg, result_path, weights=None):\n",
    "    \"\"\"\n",
    "    Prints and saves final metrics. Optionally prints top and bottom weights.\n",
    "    Does not require vector_words.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Write the results into the file\n",
    "    with open(result_path, \"w\") as text_file:\n",
    "        error_rate, false_negative, false_positive = error_rates\n",
    "\n",
    "        # Print regularization and error rates to console\n",
    "        print(f\"Regularization = {reg}, error_rate, false_negative, false_positive: \"\n",
    "              f\"{error_rate}, {false_negative}, {false_positive}\")\n",
    "\n",
    "        # Always write these to the file\n",
    "        text_file.write(f\"Regularization = {reg}, error_rate, false_negative, false_positive: \"\n",
    "                        f\"{error_rate}, {false_negative}, {false_positive}\\n\")\n",
    "\n",
    "        # Optionally print top/bottom weights if 'weights' is provided\n",
    "        if weights is not None:\n",
    "            # Sort indices of weights in ascending order\n",
    "            max_indices = np.argsort(weights)\n",
    "\n",
    "            print(\"Max values:\")\n",
    "            # If you consider weights[0] as 'bias', you can print it explicitly:\n",
    "            print(\"  bias:\", weights[0])\n",
    "\n",
    "            # Print top 5 positive weights (excluding the 0th if it's bias)\n",
    "            for i in range(1, 6):\n",
    "                idx = max_indices[-i]\n",
    "                print(f\"  Index {idx}, weight {weights[idx]}\")\n",
    "\n",
    "            print(\"Min values:\")\n",
    "            for i in range(5):\n",
    "                idx = max_indices[i]\n",
    "                print(f\"  Index {idx}, weight {weights[idx]}\")\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    #Ground truth ie correct labels\n",
    "    true_classes = labels[:, 0] \n",
    "\n",
    "    # Compute and plot confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(labels, soft_probs, save_path, csv_path, model_name):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    # Extract ground truth and predicted probabilities\n",
    "    true_classes = labels[:, 0]  # First column is '1' for positive class\n",
    "    pred_prob_class1 = soft_probs[:, 0]  # Class 1 probabilities\n",
    "    \n",
    "    # Compute ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(true_classes, pred_prob_class1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Save ROC values to CSV\n",
    "    df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'model': model_name, 'auc': roc_auc})\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)  # Append without headers\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)  # Create new file with headers\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')  # Random baseline\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc41ce-8625-4e97-bbbd-d0f159840895",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393c0328-a652-46e1-8498-2232d9b15a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "\n",
    "    def __init__(self, input_dim, hidden_layer1_dim, hidden_layer2_dim, output_dim, regularization=0.5, lr=1e-3):\n",
    "        self.reg = regularization\n",
    "        self.start_lr = lr\n",
    "        self.lr = lr\n",
    "        self.weights = {}\n",
    "        self.weights[\"layer_1\"] = np.random.normal(size=(input_dim, hidden_layer1_dim))\n",
    "        self.weights[\"bias_1\"] = np.zeros(hidden_layer1_dim)\n",
    "        self.weights[\"layer_2\"] = np.random.normal(size=(hidden_layer1_dim, hidden_layer2_dim))\n",
    "        self.weights[\"bias_2\"] = np.zeros(hidden_layer2_dim)\n",
    "        self.weights[\"layer_3\"] = np.random.normal(size=(hidden_layer2_dim, output_dim))\n",
    "        self.weights[\"bias_3\"] = np.zeros(output_dim)\n",
    "\n",
    "        print(\"self.weights[layer_1].shape\", self.weights[\"layer_1\"].shape)\n",
    "        print(\"self.weights[bias_1].shape\", self.weights[\"bias_1\"].shape)\n",
    "        print(\"self.weights[layer_2].shape\", self.weights[\"layer_2\"].shape)\n",
    "        print(\"self.weights[bias_2].shape\", self.weights[\"bias_2\"].shape)\n",
    "        print(\"self.weights[layer_3].shape\", self.weights[\"layer_3\"].shape)\n",
    "        print(\"self.weights[bias_3].shape\", self.weights[\"bias_3\"].shape)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        max_value = np.max(x, axis=1, keepdims=True)\n",
    "        exp_matrix = np.exp(np.subtract(x, max_value))  # subtract max for numerical stability\n",
    "        sum_vector = np.sum(exp_matrix, axis=1, keepdims=True)\n",
    "        softmax_x = exp_matrix / sum_vector\n",
    "        return softmax_x\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        layer1 = np.matmul(x, self.weights[\"layer_1\"]) + self.weights[\"bias_1\"]\n",
    "        sig1 = self.sigmoid(layer1)\n",
    "        layer2 = np.matmul(sig1, self.weights[\"layer_2\"]) + self.weights[\"bias_2\"]\n",
    "        sig2 = self.sigmoid(layer2)\n",
    "        layer3 = np.matmul(sig2, self.weights[\"layer_3\"]) + self.weights[\"bias_3\"]\n",
    "        soft3 = self.softmax(layer3)\n",
    "        loss_list = [np.log(np.matmul(soft3[i], label) + 1e-10) for i, label in enumerate(labels)]\n",
    "        loss = -np.mean(loss_list)\n",
    "        return sig1, sig2, soft3, loss\n",
    "    \n",
    "    def backward(self, x, labels):\n",
    "        sig1, sig2, soft3, _ = self.forward(x, labels)\n",
    "        batch_lenth = len(labels)\n",
    "\n",
    "        dO = soft3 - labels\n",
    "        dB3 = np.sum(dO, axis=0) / batch_lenth\n",
    "        dL3 = np.matmul(sig2.T, dO) / batch_lenth + 2 * self.reg * self.weights[\"layer_3\"]\n",
    "\n",
    "        dsig2 = sig2 * (1 - sig2) * np.matmul(dO, self.weights[\"layer_3\"].T)\n",
    "        dB2 = np.sum(dsig2, axis=0) / batch_lenth\n",
    "        dL2 = np.matmul(sig1.T, dsig2) / batch_lenth + 2 * self.reg * self.weights[\"layer_2\"]\n",
    "\n",
    "        dsig = sig1 * (1 - sig1) * np.matmul(dsig2, self.weights[\"layer_2\"].T)\n",
    "        dB1 = np.sum(dsig, axis=0) / batch_lenth\n",
    "        dL1 = np.matmul(x.T, dsig) / batch_lenth + 2 * self.reg * self.weights[\"layer_1\"]\n",
    "        \n",
    "        return dL1, dB1, dL2, dB2, dL3, dB3\n",
    "    \n",
    "    def batch_gradient_descent(self, x, labels, batch_dim, epoch):\n",
    "        indices = np.random.permutation(np.arange(len(labels)))\n",
    "        for i in range(0, len(x), batch_dim):\n",
    "            batch_indices = indices[i:i + batch_dim]\n",
    "            data_batch = x[batch_indices]\n",
    "            label_batch = labels[batch_indices]\n",
    "            dL1, dB1, dL2, dB2, dL3, dB3 = self.backward(data_batch, label_batch)\n",
    "\n",
    "            self.weights[\"layer_1\"] -= dL1 * self.lr\n",
    "            self.weights[\"bias_1\"] -= dB1 * self.lr\n",
    "            self.weights[\"layer_2\"] -= dL2 * self.lr\n",
    "            self.weights[\"bias_2\"] -= dB2 * self.lr\n",
    "            self.weights[\"layer_3\"] -= dL3 * self.lr\n",
    "            self.weights[\"bias_3\"] -= dB3 * self.lr\n",
    "\n",
    "    def train(self, train_x, train_labels, val_x, val_labels, total_epochs, batch_dim=400):\n",
    "        if len(train_x) % batch_dim != 0:\n",
    "            raise ValueError(\"dataset must be divisible by batch_dim\")\n",
    "\n",
    "        train_loss, train_error_rates = [None], [[None, None, None]]\n",
    "        val_loss, val_error_rates = [None], [[None, None, None]]\n",
    "        \n",
    "        for epoch in range(total_epochs):\n",
    "            self.batch_gradient_descent(train_x, train_labels, batch_dim, epoch)\n",
    "\n",
    "            _, _, soft3, train_loss1 = self.forward(train_x, train_labels)\n",
    "            train_loss.append(train_loss1)\n",
    "\n",
    "            pred_list_train, train_e = self.predict(soft3, train_labels)\n",
    "            train_error_rates.append(train_e)\n",
    "\n",
    "            _, _, soft3, val_loss1 = self.forward(val_x, val_labels)\n",
    "            val_loss.append(val_loss1)\n",
    "            pred_list, val_e = self.predict(soft3, val_labels)\n",
    "            val_error_rates.append(val_e)\n",
    "\n",
    "            self.lr = self.start_lr * 1 / (epoch / 100 + 1)\n",
    "\n",
    "        return (np.array(val_loss[1:]), np.array(train_loss[1:]), \n",
    "                np.array(val_error_rates[1:]), np.array(train_error_rates[1:]))\n",
    "    \n",
    "    def predict(self, soft2, labels):\n",
    "        prediction_list = [0 if soft2[i][0] < 0.5 else 1 for i in range(len(labels))]\n",
    "        errorlist = [\n",
    "            [0, 0, 0] if labels[i][0] == [prediction_list[i]] else [1, labels[i][0], 1 - labels[i][0]]\n",
    "            for i in range(len(labels))\n",
    "        ]\n",
    "        return prediction_list, np.sum(errorlist, axis=0) / len(errorlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b841e9-e502-48ac-a609-f6954a085464",
   "metadata": {},
   "source": [
    "### read embeddings\n",
    "Run appropriate cell based on which dataset we're running for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a6d596-c2fd-4761-b06a-1dcb6f0c152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"alldata/note\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351713cd",
   "metadata": {},
   "source": [
    "#### 1. Pre-trained Bert hierarchical full text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "266a8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"BertFullTextHier\"\n",
    "file_name = \"semantic_BertHier_FullText.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927fb98",
   "metadata": {},
   "source": [
    "#### 2. Finetuned Bert full text embeddings (Cross Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928d183f-bd76-4d54-8037-e895368274ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"10krows_BertFinetuned1\"\n",
    "file_name = \"fineTuned_semantic_Bert_FullText.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb210c",
   "metadata": {},
   "source": [
    "#### 3. Finetuned Bert full text embeddings (Cosine Similarity Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd8afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"10krows_BertFinetuned2\"\n",
    "file_name = \"fineTuned_semantic_Bert_Cosine.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037afa3c",
   "metadata": {},
   "source": [
    "Sample and ensure balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f65378c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample 5000 rows from each class\n",
    "df0 = df[df['Y'] == 0].sample(n=5000, random_state=42)\n",
    "df1 = df[df['Y'] == 1].sample(n=5000, random_state=42)\n",
    "sampled_df = pd.concat([df0, df1]).reset_index(drop=True)\n",
    "print(sampled_df['Y'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23dd7dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sampled_df\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b4744",
   "metadata": {},
   "source": [
    "#### 4. OPENAI 500 rows full text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ba0c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"OpenAIfulltext\"\n",
    "file_name = \"OPENAI_merged_500embedding.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e8a6b",
   "metadata": {},
   "source": [
    "#### 5. OPENAI 500 rows summary embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b01be993",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"OpenAIsummary\"\n",
    "file_name = \"OPENAI_merged_500_summary_embedding.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b690fb77-a1a0-4932-9959-2d5d083bd875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186606</td>\n",
       "      <td>0.085287</td>\n",
       "      <td>-0.136578</td>\n",
       "      <td>0.114657</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>-0.039044</td>\n",
       "      <td>0.111851</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.084644</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015664</td>\n",
       "      <td>-0.259481</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.030835</td>\n",
       "      <td>0.149914</td>\n",
       "      <td>0.223261</td>\n",
       "      <td>0.122565</td>\n",
       "      <td>-0.019840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206593</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.170108</td>\n",
       "      <td>0.071886</td>\n",
       "      <td>-0.061216</td>\n",
       "      <td>0.113776</td>\n",
       "      <td>0.092842</td>\n",
       "      <td>0.062146</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015256</td>\n",
       "      <td>-0.130282</td>\n",
       "      <td>-0.003898</td>\n",
       "      <td>0.104951</td>\n",
       "      <td>-0.032151</td>\n",
       "      <td>0.072405</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.109439</td>\n",
       "      <td>-0.069683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.186606  0.085287 -0.136578  0.114657  0.078886 -0.039044  0.111851   \n",
       "1  0.206593  0.026372  0.024923  0.170108  0.071886 -0.061216  0.113776   \n",
       "\n",
       "          7         8         9  ...       759       760       761       762  \\\n",
       "0  0.070599  0.084644 -0.019350  ...  0.015664 -0.259481  0.042405  0.033597   \n",
       "1  0.092842  0.062146  0.004418  ... -0.015256 -0.130282 -0.003898  0.104951   \n",
       "\n",
       "        763       764       765       766       767  Y  \n",
       "0  0.030835  0.149914  0.223261  0.122565 -0.019840  1  \n",
       "1 -0.032151  0.072405  0.173873  0.109439 -0.069683  1  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd025d",
   "metadata": {},
   "source": [
    "#### appropriately update file structure to save in corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee944dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder names based on suffix\n",
    "results_folder = f\"results_{suffix}\"\n",
    "saved_weights_folder = f\"saved_weights_{suffix}\"\n",
    "\n",
    "#Ensure directories exist\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "os.makedirs(saved_weights_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4d1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last column is Y, so let's separate features (all but last column) and labels (last column)\n",
    "data = df.iloc[:, :-1].values\n",
    "labels_raw = df.iloc[:, -1].values  # shape (n_samples,)\n",
    "\n",
    "# Convert each label into a 2D form: [label, 1 - label]\n",
    "labels = np.array([[lbl, 1 - lbl] for lbl in labels_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5af8c-bd3b-4d25-81c0-720b38ca341d",
   "metadata": {},
   "source": [
    "### train model\n",
    "- Saved weights parameter -> use/ not use cached weights from prior run\n",
    "- batch_dim depends on dataset size\n",
    "- openAI datasets don't converge with learning rates 0.5, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "441cc525-587c-4fd7-9311-3d78bb5e178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weights[layer_1].shape (768, 50)\n",
      "self.weights[bias_1].shape (50,)\n",
      "self.weights[layer_2].shape (50, 50)\n",
      "self.weights[bias_2].shape (50,)\n",
      "self.weights[layer_3].shape (50, 2)\n",
      "self.weights[bias_3].shape (2,)\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Regularization: 0.0025\n",
      "Error Rate: 0.366\n",
      "False Negative: 0.123\n",
      "False Positive: 0.244\n"
     ]
    }
   ],
   "source": [
    "# We know our feature dimension is simply the number of columns (except Y)\n",
    "feature_size = data.shape[1]\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_layer1_dim = 50\n",
    "hidden_layer2_dim = 50\n",
    "learning_rate = 0.01\n",
    "regularization_terms = [0.0025] \n",
    "max_epoch = 1000\n",
    "batch_dim = 400  #fulltext hier Bert or any dataset with 10k rows\n",
    "#batch_dim = 1\n",
    "\n",
    "saved_parameters = False  # or False\n",
    "k_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "for reg in regularization_terms:\n",
    "    NN = Neural_Net(feature_size, hidden_layer1_dim, hidden_layer2_dim, 2, reg, learning_rate)\n",
    "\n",
    "    if saved_parameters:\n",
    "        \n",
    "        #Load the *full checkpoint* (weights + logs)\n",
    "        loaded_dict = np.load(f\"{saved_weights_folder}/checkpoint_{reg}.npy\", allow_pickle=True).item()\n",
    "        NN.weights = loaded_dict[\"weights\"]\n",
    "        val_loss = loaded_dict[\"val_loss\"]\n",
    "        train_loss = loaded_dict[\"train_loss\"]\n",
    "        val_error_rates = loaded_dict[\"val_error_rates\"]\n",
    "        train_error_rates = loaded_dict[\"train_error_rates\"]\n",
    "\n",
    "        #Evaluate on validation set\n",
    "        val_set, val_labels = data, labels  # Use full dataset for evaluation\n",
    "        _, _, soft3_val, val_loss_val = NN.forward(val_set, val_labels)\n",
    "        pred_list_val, val_e = NN.predict(soft3_val, val_labels)\n",
    "\n",
    "    else:\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        X, Y = np.array(data), np.array(labels)\n",
    "\n",
    "        # Store performance across folds\n",
    "        all_val_losses, all_train_losses = [], []\n",
    "        all_val_errors, all_train_errors = [], []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "            # Split data into train and validation for this fold\n",
    "            X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "            X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "            #batch_dim = len(X_train)\n",
    "\n",
    "            # Train on the fold\n",
    "            val_loss, train_loss, val_error_rates, train_error_rates = NN.train(\n",
    "                X_train, Y_train,\n",
    "                X_val, Y_val,\n",
    "                max_epoch, batch_dim\n",
    "            )\n",
    "\n",
    "            # Store results from this fold\n",
    "            all_val_losses.append(val_loss)\n",
    "            all_train_losses.append(train_loss)\n",
    "            all_val_errors.append(val_error_rates)\n",
    "            all_train_errors.append(train_error_rates)\n",
    "\n",
    "            # Ensure we have a final validation set after cross-validation\n",
    "            val_set, val_labels = X_val, Y_val  # Save last fold's val set\n",
    "\n",
    "        #Average results across folds\n",
    "        val_loss = np.mean(all_val_losses, axis=0)\n",
    "        train_loss = np.mean(all_train_losses, axis=0)\n",
    "        val_error_rates = np.mean(all_val_errors, axis=0)\n",
    "        train_error_rates = np.mean(all_train_errors, axis=0)\n",
    "\n",
    "        #Evaluate on validation set after all folds\n",
    "        _, _, soft3_val, _ = NN.forward(val_set, val_labels)\n",
    "        pred_list_val, _ = NN.predict(soft3_val, val_labels)\n",
    "\n",
    "        #Save trained weights & logs\n",
    "        np.save(f\"{saved_weights_folder}/weights_{reg}.npy\", NN.weights)\n",
    "\n",
    "        save_dict = {\n",
    "            \"weights\": NN.weights,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_error_rates\": val_error_rates,\n",
    "            \"train_error_rates\": train_error_rates\n",
    "        }\n",
    "        np.save(f\"{saved_weights_folder}/checkpoint_{reg}.npy\", save_dict)\n",
    "\n",
    "    #Shared code: plots & confusion matrix & ROC\n",
    "    plot_training(\n",
    "        val_loss, \"val_loss\",\n",
    "        train_loss, \"train_loss\",\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_loss.png\"\n",
    "    )\n",
    "\n",
    "    plot_training(\n",
    "        val_error_rates[:, 0], \"val_error\",\n",
    "        train_error_rates[:, 0], \"train_error\",\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_errors.png\"\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        val_labels,\n",
    "        pred_list_val,\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_confusion.png\"\n",
    "    )\n",
    "\n",
    "    plot_roc_curve(\n",
    "        val_labels,\n",
    "        soft3_val,\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_roc.png\",\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_roc.csv\",\n",
    "        suffix,\n",
    "    )\n",
    "\n",
    "    # Print final validation error rates\n",
    "    error_rate, false_negative, false_positive = val_error_rates[-1]\n",
    "    print(f\"Regularization: {reg}\")\n",
    "    print(f\"Error Rate: {error_rate:.3f}\")\n",
    "    print(f\"False Negative: {false_negative:.3f}\")\n",
    "    print(f\"False Positive: {false_positive:.3f}\")\n",
    "\n",
    "    # Write results to a txt file\n",
    "    with open(f\"{results_folder}/NeuralNet_reg{reg}.txt\", \"w\") as text_file:\n",
    "        text_file.write(f\"Regularization: {reg}\\n\")\n",
    "        text_file.write(f\"Error Rate: {error_rate:.3f}\\n\")\n",
    "        text_file.write(f\"False Negative: {false_negative:.3f}\\n\")\n",
    "        text_file.write(f\"False Positive: {false_positive:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614b561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

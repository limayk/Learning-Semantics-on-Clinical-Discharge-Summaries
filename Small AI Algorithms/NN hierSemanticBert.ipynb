{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ee5d720-857c-4242-99de-3286458162d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed222c-50fa-481d-874c-6ffde954fc0d",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90c7d244-a33f-4719-b195-012b4102a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_intercept(x):\n",
    "\n",
    "    with_intercept = np.zeros((len(x), len(x[0])+1), dtype = x.dtype)\n",
    "    with_intercept[:, 0] = 1\n",
    "    with_intercept[:, 1:] = x\n",
    "\n",
    "    return with_intercept\n",
    "\n",
    "def plot_training(list1, name_of_list1, list2, name_of_list2, save_path):\n",
    "    x_values = np.linspace(1, len(list1), len(list1))\n",
    "    plt.plot(x_values, list1, label=name_of_list1)\n",
    "    plt.plot(x_values, list2, label=name_of_list2)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def print_results(error_rates, reg, result_path, weights=None):\n",
    "    \"\"\"\n",
    "    Prints and saves final metrics. Optionally prints top and bottom weights.\n",
    "    Does not require vector_words.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Write the results into the file\n",
    "    with open(result_path, \"w\") as text_file:\n",
    "        error_rate, false_negative, false_positive = error_rates\n",
    "\n",
    "        # Print regularization and error rates to console\n",
    "        print(f\"Regularization = {reg}, error_rate, false_negative, false_positive: \"\n",
    "              f\"{error_rate}, {false_negative}, {false_positive}\")\n",
    "\n",
    "        # Always write these to the file\n",
    "        text_file.write(f\"Regularization = {reg}, error_rate, false_negative, false_positive: \"\n",
    "                        f\"{error_rate}, {false_negative}, {false_positive}\\n\")\n",
    "\n",
    "        # Optionally print top/bottom weights if 'weights' is provided\n",
    "        if weights is not None:\n",
    "            # Sort indices of weights in ascending order\n",
    "            max_indices = np.argsort(weights)\n",
    "\n",
    "            print(\"Max values:\")\n",
    "            # If you consider weights[0] as 'bias', you can print it explicitly:\n",
    "            print(\"  bias:\", weights[0])\n",
    "\n",
    "            # Print top 5 positive weights (excluding the 0th if it's bias)\n",
    "            for i in range(1, 6):\n",
    "                idx = max_indices[-i]\n",
    "                print(f\"  Index {idx}, weight {weights[idx]}\")\n",
    "\n",
    "            print(\"Min values:\")\n",
    "            for i in range(5):\n",
    "                idx = max_indices[i]\n",
    "                print(f\"  Index {idx}, weight {weights[idx]}\")\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions, save_path):\n",
    "    \"\"\"\n",
    "    Plots and saves a confusion matrix heatmap.\n",
    "    :param labels: one-hot ground-truth labels, shape (N,2).\n",
    "    :param predictions: predicted class indices (0 or 1), length N.\n",
    "    :param save_path: path to save the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    #Ground truth ie correct labels\n",
    "    true_classes = labels[:, 0] \n",
    "\n",
    "    # Compute and plot confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_roc_curve(labels, soft_probs, save_path):\n",
    "    \"\"\"\n",
    "    Plots and saves an ROC curve with AUC.\n",
    "    :param labels: one-hot ground-truth labels, shape (N,2).\n",
    "    :param soft_probs: predicted probabilities for each class, shape (N,2).\n",
    "                      soft_probs[:,1] is the probability of class '1' (positive class).\n",
    "    :param save_path: path to save the ROC plot.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Create folder if needed\n",
    "\n",
    "    # Ground truth: directly use labels[:, 0] (first column is '1' for positive class)\n",
    "    true_classes = labels[:, 0]  \n",
    "    print(\"labels: \")\n",
    "    print(labels)\n",
    "    print(true_classes)\n",
    "\n",
    "    # Correct class-1 probabilities\n",
    "    pred_prob_class1 = soft_probs[:, 0]  \n",
    "    print(\"soft prob: \")\n",
    "    print(soft_probs)\n",
    "    print(pred_prob_class1)\n",
    "\n",
    "    # Compute ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(true_classes, pred_prob_class1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')  # Random baseline\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc41ce-8625-4e97-bbbd-d0f159840895",
   "metadata": {},
   "source": [
    "#### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "393c0328-a652-46e1-8498-2232d9b15a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "\n",
    "    def __init__(self, input_dim, hidden_layer1_dim, hidden_layer2_dim, output_dim, regularization=0.5, lr=1e-3):\n",
    "        self.reg = regularization\n",
    "        self.start_lr = lr\n",
    "        self.lr = lr\n",
    "        self.weights = {}\n",
    "        self.weights[\"layer_1\"] = np.random.normal(size=(input_dim, hidden_layer1_dim))\n",
    "        self.weights[\"bias_1\"] = np.zeros(hidden_layer1_dim)\n",
    "        self.weights[\"layer_2\"] = np.random.normal(size=(hidden_layer1_dim, hidden_layer2_dim))\n",
    "        self.weights[\"bias_2\"] = np.zeros(hidden_layer2_dim)\n",
    "        self.weights[\"layer_3\"] = np.random.normal(size=(hidden_layer2_dim, output_dim))\n",
    "        self.weights[\"bias_3\"] = np.zeros(output_dim)\n",
    "\n",
    "        print(\"self.weights[layer_1].shape\", self.weights[\"layer_1\"].shape)\n",
    "        print(\"self.weights[bias_1].shape\", self.weights[\"bias_1\"].shape)\n",
    "        print(\"self.weights[layer_2].shape\", self.weights[\"layer_2\"].shape)\n",
    "        print(\"self.weights[bias_2].shape\", self.weights[\"bias_2\"].shape)\n",
    "        print(\"self.weights[layer_3].shape\", self.weights[\"layer_3\"].shape)\n",
    "        print(\"self.weights[bias_3].shape\", self.weights[\"bias_3\"].shape)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        max_value = np.max(x, axis=1, keepdims=True)\n",
    "        exp_matrix = np.exp(np.subtract(x, max_value))  # subtract max for numerical stability\n",
    "        sum_vector = np.sum(exp_matrix, axis=1, keepdims=True)\n",
    "        softmax_x = exp_matrix / sum_vector\n",
    "        return softmax_x\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        layer1 = np.matmul(x, self.weights[\"layer_1\"]) + self.weights[\"bias_1\"]\n",
    "        sig1 = self.sigmoid(layer1)\n",
    "        layer2 = np.matmul(sig1, self.weights[\"layer_2\"]) + self.weights[\"bias_2\"]\n",
    "        sig2 = self.sigmoid(layer2)\n",
    "        layer3 = np.matmul(sig2, self.weights[\"layer_3\"]) + self.weights[\"bias_3\"]\n",
    "        soft3 = self.softmax(layer3)\n",
    "        loss_list = [np.log(np.matmul(soft3[i], label) + 1e-10) for i, label in enumerate(labels)]\n",
    "        loss = -np.mean(loss_list)\n",
    "        return sig1, sig2, soft3, loss\n",
    "    \n",
    "    def backward(self, x, labels):\n",
    "        sig1, sig2, soft3, _ = self.forward(x, labels)\n",
    "        batch_lenth = len(labels)\n",
    "\n",
    "        dO = soft3 - labels\n",
    "        dB3 = np.sum(dO, axis=0) / batch_lenth\n",
    "        dL3 = np.matmul(sig2.T, dO) / batch_lenth + 2 * self.reg * self.weights[\"layer_3\"]\n",
    "\n",
    "        dsig2 = sig2 * (1 - sig2) * np.matmul(dO, self.weights[\"layer_3\"].T)\n",
    "        dB2 = np.sum(dsig2, axis=0) / batch_lenth\n",
    "        dL2 = np.matmul(sig1.T, dsig2) / batch_lenth + 2 * self.reg * self.weights[\"layer_2\"]\n",
    "\n",
    "        dsig = sig1 * (1 - sig1) * np.matmul(dsig2, self.weights[\"layer_2\"].T)\n",
    "        dB1 = np.sum(dsig, axis=0) / batch_lenth\n",
    "        dL1 = np.matmul(x.T, dsig) / batch_lenth + 2 * self.reg * self.weights[\"layer_1\"]\n",
    "        \n",
    "        return dL1, dB1, dL2, dB2, dL3, dB3\n",
    "    \n",
    "    def batch_gradient_descent(self, x, labels, batch_dim, epoch):\n",
    "        indices = np.random.permutation(np.arange(len(labels)))\n",
    "        for i in range(0, len(x), batch_dim):\n",
    "            batch_indices = indices[i:i + batch_dim]\n",
    "            data_batch = x[batch_indices]\n",
    "            label_batch = labels[batch_indices]\n",
    "            dL1, dB1, dL2, dB2, dL3, dB3 = self.backward(data_batch, label_batch)\n",
    "\n",
    "            self.weights[\"layer_1\"] -= dL1 * self.lr\n",
    "            self.weights[\"bias_1\"] -= dB1 * self.lr\n",
    "            self.weights[\"layer_2\"] -= dL2 * self.lr\n",
    "            self.weights[\"bias_2\"] -= dB2 * self.lr\n",
    "            self.weights[\"layer_3\"] -= dL3 * self.lr\n",
    "            self.weights[\"bias_3\"] -= dB3 * self.lr\n",
    "\n",
    "    def train(self, train_x, train_labels, val_x, val_labels, total_epochs, batch_dim=400):\n",
    "        if len(train_x) % batch_dim != 0:\n",
    "            raise ValueError(\"dataset must be divisible by batch_dim\")\n",
    "\n",
    "        train_loss, train_error_rates = [None], [[None, None, None]]\n",
    "        val_loss, val_error_rates = [None], [[None, None, None]]\n",
    "        \n",
    "        for epoch in range(total_epochs):\n",
    "            self.batch_gradient_descent(train_x, train_labels, batch_dim, epoch)\n",
    "\n",
    "            _, _, soft3, train_loss1 = self.forward(train_x, train_labels)\n",
    "            train_loss.append(train_loss1)\n",
    "\n",
    "            pred_list_train, train_e = self.predict(soft3, train_labels)\n",
    "            train_error_rates.append(train_e)\n",
    "\n",
    "            _, _, soft3, val_loss1 = self.forward(val_x, val_labels)\n",
    "            val_loss.append(val_loss1)\n",
    "            pred_list, val_e = self.predict(soft3, val_labels)\n",
    "            val_error_rates.append(val_e)\n",
    "\n",
    "            #print(f\"Epoch: {epoch}/{total_epochs}, ValLoss:{val_loss[epoch]}, TrainLoss:{train_loss[epoch]}, \"\n",
    "             #     f\"ValError%:{val_error_rates[epoch][0]}, TrainError%:{train_error_rates[epoch][0]}\")\n",
    "            self.lr = self.start_lr * 1 / (epoch / 100 + 1)\n",
    "\n",
    "        return (np.array(val_loss[1:]), np.array(train_loss[1:]), \n",
    "                np.array(val_error_rates[1:]), np.array(train_error_rates[1:]))\n",
    "    \n",
    "    def predict(self, soft2, labels):\n",
    "        prediction_list = [0 if soft2[i][0] < 0.5 else 1 for i in range(len(labels))]\n",
    "        errorlist = [\n",
    "            [0, 0, 0] if labels[i][0] == [prediction_list[i]] else [1, labels[i][0], 1 - labels[i][0]]\n",
    "            for i in range(len(labels))\n",
    "        ]\n",
    "        return prediction_list, np.sum(errorlist, axis=0) / len(errorlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b841e9-e502-48ac-a609-f6954a085464",
   "metadata": {},
   "source": [
    "#### read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4a6d596-c2fd-4761-b06a-1dcb6f0c152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"alldata/note\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351713cd",
   "metadata": {},
   "source": [
    "Bert direct full text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "266a8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"BertFullTextHier\"\n",
    "file_name = \"semantic_BertHier_FullText.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927fb98",
   "metadata": {},
   "source": [
    "Bert finetuned full text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928d183f-bd76-4d54-8037-e895368274ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"BertFinetuned1\"\n",
    "file_name = \"fineTuned_semantic_Bert_FullText.csv\"\n",
    "file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b4744",
   "metadata": {},
   "source": [
    "OPENAI 500 rows full text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b690fb77-a1a0-4932-9959-2d5d083bd875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186606</td>\n",
       "      <td>0.085287</td>\n",
       "      <td>-0.136578</td>\n",
       "      <td>0.114657</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>-0.039044</td>\n",
       "      <td>0.111851</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.084644</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015664</td>\n",
       "      <td>-0.259481</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.030835</td>\n",
       "      <td>0.149914</td>\n",
       "      <td>0.223261</td>\n",
       "      <td>0.122565</td>\n",
       "      <td>-0.019840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206593</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.170108</td>\n",
       "      <td>0.071886</td>\n",
       "      <td>-0.061216</td>\n",
       "      <td>0.113776</td>\n",
       "      <td>0.092842</td>\n",
       "      <td>0.062146</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015256</td>\n",
       "      <td>-0.130282</td>\n",
       "      <td>-0.003898</td>\n",
       "      <td>0.104951</td>\n",
       "      <td>-0.032151</td>\n",
       "      <td>0.072405</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.109439</td>\n",
       "      <td>-0.069683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.186606  0.085287 -0.136578  0.114657  0.078886 -0.039044  0.111851   \n",
       "1  0.206593  0.026372  0.024923  0.170108  0.071886 -0.061216  0.113776   \n",
       "\n",
       "          7         8         9  ...       759       760       761       762  \\\n",
       "0  0.070599  0.084644 -0.019350  ...  0.015664 -0.259481  0.042405  0.033597   \n",
       "1  0.092842  0.062146  0.004418  ... -0.015256 -0.130282 -0.003898  0.104951   \n",
       "\n",
       "        763       764       765       766       767  Y  \n",
       "0  0.030835  0.149914  0.223261  0.122565 -0.019840  1  \n",
       "1 -0.032151  0.072405  0.173873  0.109439 -0.069683  1  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd025d",
   "metadata": {},
   "source": [
    "Appropriately update file structure to save in corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bee944dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder names based on suffix\n",
    "results_folder = f\"results_{suffix}\"\n",
    "saved_weights_folder = f\"saved_weights_{suffix}\"\n",
    "\n",
    "#Ensure directories exist\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "os.makedirs(saved_weights_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a4d1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last column is Y, so let's separate features (all but last column) and labels (last column)\n",
    "data = df.iloc[:, :-1].values\n",
    "labels_raw = df.iloc[:, -1].values  # shape (n_samples,)\n",
    "\n",
    "# Convert each label into a 2D form: [label, 1 - label]\n",
    "labels = np.array([[lbl, 1 - lbl] for lbl in labels_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5af8c-bd3b-4d25-81c0-720b38ca341d",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "441cc525-587c-4fd7-9311-3d78bb5e178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weights[layer_1].shape (768, 50)\n",
      "self.weights[bias_1].shape (50,)\n",
      "self.weights[layer_2].shape (50, 50)\n",
      "self.weights[bias_2].shape (50,)\n",
      "self.weights[layer_3].shape (50, 2)\n",
      "self.weights[bias_3].shape (2,)\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "labels: \n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[1 0 1 ... 0 0 1]\n",
      "soft prob: \n",
      "[[0.51330915 0.48669085]\n",
      " [0.42847894 0.57152106]\n",
      " [0.47799439 0.52200561]\n",
      " ...\n",
      " [0.47857229 0.52142771]\n",
      " [0.51741149 0.48258851]\n",
      " [0.64226348 0.35773652]]\n",
      "[0.51330915 0.42847894 0.47799439 ... 0.47857229 0.51741149 0.64226348]\n",
      "Regularization: 0.0025\n",
      "Error Rate: 0.358\n",
      "False Negative: 0.120\n",
      "False Positive: 0.238\n"
     ]
    }
   ],
   "source": [
    "# We know our feature dimension is simply the number of columns (except Y)\n",
    "feature_size = data.shape[1]\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_layer1_dim = 50\n",
    "hidden_layer2_dim = 50\n",
    "learning_rate = 0.5\n",
    "regularization_terms = [0.0025]  # Loop over multiple\n",
    "max_epoch = 1000\n",
    "batch_dim = 400  # fulltext hier Bert\n",
    "\n",
    "saved_parameters = False  # or False\n",
    "k_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "for reg in regularization_terms:\n",
    "    NN = Neural_Net(feature_size, hidden_layer1_dim, hidden_layer2_dim, 2, reg, learning_rate)\n",
    "\n",
    "    if saved_parameters:\n",
    "        \n",
    "        #Load the *full checkpoint* (weights + logs)\n",
    "        loaded_dict = np.load(f\"{saved_weights_folder}/checkpoint_{reg}.npy\", allow_pickle=True).item()\n",
    "        NN.weights = loaded_dict[\"weights\"]\n",
    "        val_loss = loaded_dict[\"val_loss\"]\n",
    "        train_loss = loaded_dict[\"train_loss\"]\n",
    "        val_error_rates = loaded_dict[\"val_error_rates\"]\n",
    "        train_error_rates = loaded_dict[\"train_error_rates\"]\n",
    "\n",
    "        #Evaluate on validation set\n",
    "        val_set, val_labels = data, labels  # Use full dataset for evaluation\n",
    "        _, _, soft3_val, val_loss_val = NN.forward(val_set, val_labels)\n",
    "        pred_list_val, val_e = NN.predict(soft3_val, val_labels)\n",
    "\n",
    "    else:\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        X, Y = np.array(data), np.array(labels)\n",
    "\n",
    "        # Store performance across folds\n",
    "        all_val_losses, all_train_losses = [], []\n",
    "        all_val_errors, all_train_errors = [], []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "            # Split data into train and validation for this fold\n",
    "            X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "            X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "\n",
    "            # Train on the fold\n",
    "            val_loss, train_loss, val_error_rates, train_error_rates = NN.train(\n",
    "                X_train, Y_train,\n",
    "                X_val, Y_val,\n",
    "                max_epoch, batch_dim\n",
    "            )\n",
    "\n",
    "            # Store results from this fold\n",
    "            all_val_losses.append(val_loss)\n",
    "            all_train_losses.append(train_loss)\n",
    "            all_val_errors.append(val_error_rates)\n",
    "            all_train_errors.append(train_error_rates)\n",
    "\n",
    "            # Ensure we have a final validation set after cross-validation\n",
    "            val_set, val_labels = X_val, Y_val  # Save last fold's val set\n",
    "\n",
    "        #Average results across folds\n",
    "        val_loss = np.mean(all_val_losses, axis=0)\n",
    "        train_loss = np.mean(all_train_losses, axis=0)\n",
    "        val_error_rates = np.mean(all_val_errors, axis=0)\n",
    "        train_error_rates = np.mean(all_train_errors, axis=0)\n",
    "\n",
    "        #Evaluate on validation set after all folds\n",
    "        _, _, soft3_val, _ = NN.forward(val_set, val_labels)\n",
    "        pred_list_val, _ = NN.predict(soft3_val, val_labels)\n",
    "\n",
    "        #Save trained weights & logs\n",
    "        np.save(f\"{saved_weights_folder}/weights_{reg}.npy\", NN.weights)\n",
    "\n",
    "        save_dict = {\n",
    "            \"weights\": NN.weights,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_error_rates\": val_error_rates,\n",
    "            \"train_error_rates\": train_error_rates\n",
    "        }\n",
    "        np.save(f\"{saved_weights_folder}/checkpoint_{reg}.npy\", save_dict)\n",
    "\n",
    "    #Shared code: plots & confusion matrix & ROC\n",
    "    plot_training(\n",
    "        val_loss, \"val_loss\",\n",
    "        train_loss, \"train_loss\",\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_loss.png\"\n",
    "    )\n",
    "\n",
    "    plot_training(\n",
    "        val_error_rates[:, 0], \"val_error\",\n",
    "        train_error_rates[:, 0], \"train_error\",\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_errors.png\"\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        val_labels,\n",
    "        pred_list_val,\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_confusion.png\"\n",
    "    )\n",
    "\n",
    "    plot_roc_curve(\n",
    "        val_labels,\n",
    "        soft3_val,\n",
    "        f\"{results_folder}/NeuralNet_reg{reg}_roc.png\"\n",
    "    )\n",
    "\n",
    "    # Print final validation error rates\n",
    "    error_rate, false_negative, false_positive = val_error_rates[-1]\n",
    "    print(f\"Regularization: {reg}\")\n",
    "    print(f\"Error Rate: {error_rate:.3f}\")\n",
    "    print(f\"False Negative: {false_negative:.3f}\")\n",
    "    print(f\"False Positive: {false_positive:.3f}\")\n",
    "\n",
    "    # Write results to a txt file\n",
    "    with open(f\"{results_folder}/NeuralNet_reg{reg}.txt\", \"w\") as text_file:\n",
    "        text_file.write(f\"Regularization: {reg}\\n\")\n",
    "        text_file.write(f\"Error Rate: {error_rate:.3f}\\n\")\n",
    "        text_file.write(f\"False Negative: {false_negative:.3f}\\n\")\n",
    "        text_file.write(f\"False Positive: {false_positive:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d214062e-8040-4a64-b83a-fd84e2093743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_BertFullTextHier'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e1e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
